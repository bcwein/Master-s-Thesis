@book{Balog:2018:Book,
  author = {Krisztian Balog},
  title = {Entity-Oriented Search},
  series = {The Information Retrieval Series},
  volume = {39},
  publisher = {Springer},
  year = {2018}
}

@misc{Balog:2020:arXiv,
  author = {Krisztian Balog and Lucie Flekova and Matthias Hagen and Rosie Jones and Martin Potthast and Filip Radlinski and Mark Sanderson and Svitlana Vakulenko and Hamed Zamani},
  title = {Common Conversational Community Prototype: Scholarly Conversational Assistant},
  archivePrefix = {arXiv},
  eprint = {2001.06910},
  primaryClass = {cs.CL},
  year = {2020}
}

@phdthesis{Maxwell:2019:PhDThesis,
  author = {Maxwell, David Martin},
  title = {Modelling search and stopping in interactive information retrieval},
  school = {University of Glasgow},
  year = {2019}
}  

@misc{Rasa:2022:doc,
  author = {Rasa Technologies Inc.},
  title = {Rasa Documentation},
  year = {2022},
  url = {https://rasa.com/docs/rasa/},
  note = {Accessed: 2022-01-25}
}

@article{Sanderson:2010:FnTIR,
  author = {Sanderson, Mark},
  title = {Test Collection Based Evaluation of Information Retrieval Systems},
  journal = {Found. Trends Inf. Retr.},
  volume = {4},
  number = {4},
  pages = {247--375},
  year = {2010}
}

@inproceedings{Zhang:2020:KDD,
  author = {Zhang, Shuo and Balog, Krisztian},
  title = {Evaluating Conversational Recommender Systems via User Simulation},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  series = {KDD '20},
  pages = {1512--1520},
  year = {2020}
}


@article{altman2011discrimination,
  title={Discrimination},
  author={Altman, Andrew},
  year={2011}
}


@InProceedings{pmlr-v81-binns18a,
  title = 	 {Fairness in Machine Learning: Lessons from Political Philosophy},
  author = 	 {Binns, Reuben},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {149--159},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/binns18a/binns18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/binns18a.html},
  abstract = 	 {What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.}
}

@article{dressel2018,
author = {Julia Dressel  and Hany Farid },
title = {The accuracy, fairness, and limits of predicting recidivism},
journal = {Science Advances},
volume = {4},
number = {1},
pages = {eaao5580},
year = {2018},
doi = {10.1126/sciadv.aao5580},

URL = {https://www.science.org/doi/abs/10.1126/sciadv.aao5580},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aao5580}
,
    abstract = { Should we trust computers to make life-altering decisions in the criminal justice system? Algorithms for predicting recidivism are commonly used to assess a criminal defendant’s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features. }
}

@article{calders2010three,
  title={Three naive Bayes approaches for discrimination-free classification},
  author={Calders, Toon and Verwer, Sicco},
  journal={Data mining and knowledge discovery},
  volume={21},
  number={2},
  pages={277--292},
  year={2010},
  publisher={Springer}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{kleinberg2016inherent,
  title={Inherent trade-offs in the fair determination of risk scores},
  author={Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  journal={arXiv preprint arXiv:1609.05807},
  year={2016}
}

@article{suresh2019framework,
  title={A framework for understanding unintended consequences of machine learning},
  author={Suresh, Harini and Guttag, John V},
  journal={arXiv preprint arXiv:1901.10002},
  volume={2},
  year={2019}
}

@article{olteanu2019social,
  title={Social data: Biases, methodological pitfalls, and ethical boundaries},
  author={Olteanu, Alexandra and Castillo, Carlos and Diaz, Fernando and K{\i}c{\i}man, Emre},
  journal={Frontiers in Big Data},
  volume={2},
  pages={13},
  year={2019},
  publisher={Frontiers}
}

@inproceedings{nripsuta2019,
author = {Saxena, Nripsuta Ani and Huang, Karen and DeFilippis, Evan and Radanovic, Goran and Parkes, David C. and Liu, Yang},
title = {How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314248},
doi = {10.1145/3306618.3314248},
abstract = {What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {99–106},
numpages = {8},
keywords = {fairness, public attitudes, algorithmic definition, human experiments},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{hardt2016,
 author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{NIPS2016_9d268236,
 author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Equality of Opportunity in Supervised Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{dwork2012,
author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
title = {Fairness through Awareness},
year = {2012},
isbn = {9781450311151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2090236.2090255},
doi = {10.1145/2090236.2090255},
abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
pages = {214–226},
numpages = {13},
location = {Cambridge, Massachusetts},
series = {ITCS '12}
}

@inproceedings{zafar2017,
 author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel and Gummadi, Krishna and Weller, Adrian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {From Parity to Preference-based Notions of Fairness in Classification},
 url = {https://proceedings.neurips.cc/paper/2017/file/82161242827b703e6acf9c726942a1e4-Paper.pdf},
 volume = {30},
 year = {2017}
}


@InProceedings{dwork18,
  title = 	 {Decoupled Classifiers for Group-Fair and Efficient Machine Learning},
  author = 	 {Dwork, Cynthia and Immorlica, Nicole and Kalai, Adam Tauman and Leiserson, Max},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {119--133},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/dwork18a.html},
  abstract = 	 {When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.}
}

@article{choi2020,
  author    = {YooJung Choi and
               Meihua Dang and
               Guy Van den Broeck},
  title     = {Group Fairness by Probabilistic Modeling with Latent Fair Decisions},
  journal   = {CoRR},
  volume    = {abs/2009.09031},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.09031},
  eprinttype = {arXiv},
  eprint    = {2009.09031},
  timestamp = {Wed, 23 Sep 2020 15:51:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-09031.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{murphy2012machine,
  title={Machine learning: a probabilistic perspective},
  author={Murphy, Kevin P},
  year={2012},
  publisher={MIT press}
}