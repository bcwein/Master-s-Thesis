
\chapter{Introduction}
\label{ch:intro}

\instructions{Page budget for Introduction: 3-5 pages}

\section{Background and Motivation}
\label{sec:intro:background}
Discrimination is the act of making unjustified distinctions between people based on the groups, classes, or other categories to which they belong or are perceived to belong. There exists many different definitions of discrimination \cite{Altman:2011:doc} and the fairness that one wants to achieve to avoid this discrimination in terms of machine learning. \cite{Binns:2018:PMLR}

According to \citet{Dressel:2018:AAAS}, a real world example of discrimination in terms of machine learning is COMPAS. A widely used criminal risk assessment tool. It has been used to assess more than 1 million offenders since it was developed in 1998. Compas predicts a defendant’s risk of committing a misdemeanour or felony within 2 years of assessment from 137 features about an individual and the individual’s past criminal record.

One would think that a simple way to avoid discrimination is to not consider the groups, classes, or other categories that is seen as sensitive. Counter intuitively, ignoring sensitive attributes is not enough to mitigate bias. The machine learning model learns the discriminatory decision rule indirectly from attributes that correlate to the sensitive one. \cite{Dressel:2018:AAAS, Calders:20210:DMKD} This process is called redlining. The sensitive attribute must be included to penalise the machine learning model when it discriminates.

The source of bias and discrimination often comes from the data that the machine learning algorithm is trained on. According to \citet{Mehrabi:2021:CSUR} some prominent sources of bias are

\begin{itemize}
    \item \textbf{Historical Bias}: Historical bias is the already existing bias and socio-technical issues in the world. This affect the data generation process.
    \item \textbf{Representation Bias}: Lack of representation of certain groups in datasets skew the dataset from the real-world distribution.
    \item \textbf{Simpsons Paradox}: Bias originating from the analysis of heterogeneous data that is composed of subgroups or individuals with different behaviours.
\end{itemize}

Data, especially big data, is often heterogeneous, generated by subgroups with their own characteristics and behaviors. The heterogeneity can bias the data. A model learned on biased data may lead to unfair and inaccurate predictions.

There are currently several challenges in the field of fair machine learning to face. Since ignoring the sensitive attributes does not help to mitigate bias, one must discover how to best employ these sensitive attributes to achieve fairness. In the literature there exists several statistical and mathematical definitions of fairness. Rather concerning is the fact that some of these definitions is mathematically impossible to satisfy simultaneously. \cite{Kleinberg:2017:LIPIcs}

As of the time of writing this thesis, there is no gold standard on how to train fairness-aware machine learning algorithms and there exists several approaches on how to achieve this. \cite{Mehrabi:2021:CSUR} The goal of this thesis is to explore some of these approaches and see how they compare to others. 

\instructions{
\begin{itemize}
    \item Awaken the reader's interest and convince her why the theme is important. (done)
    \item Background information might be historical in nature, or it might refer to previous research or practical considerations. (done)
    \item Provide an example or use case for the problem. (done)
    \item It should be written on a level that it's understandable by anyone with a computer science master's degree. (done)
    \item It might contain a small handful of citations if it is needed to justify some main claims or assumptions, but this is not the part to detail any related work and/or compare among each other. (done)
\end{itemize}
}

\section{Objectives}
\label{sec:intro:objectives}

The goal of this thesis is to explore certain models and approaches to achieve fairness-aware machine learning systems. Specifically, the thesis has the following objectives

\begin{itemize}
    \item Discover how probabilistic machine learning and graphical models can be used to model the discrimination process.
    \item Discover how probabilistic machine learning and graphical models can be used to quantify uncertainty in the model and it's fairness.
    \item Explore what definitions of fairness is most appropriate for probabilistic machine learning.
    \item How does the probabilistic approach compare to traditional machine learning methods?
\end{itemize}

\subsection{RQ1: What probabilistic graphical model is most appropriate to model the discrimination process?}

Probabilistic machine learning models come in many different forms. In this thesis we want to use probabilistic machine learning to model the discrimination and data generation process. This is explained in further detail in section~\ref{ch:related}. At the end, a specific probabilistic graphical model should be provided.

\subsection{RQ2: How to quantify uncertainty in model and fairness.}

Given the probabilistic (bayesian) approach that will be used in this thesis. Uncertainty and posterior distributions of model parameters could somehow be used to adress unfairness and should be investigated. Additionally, given a definition of unfairness employed by the model, a posterior distribution of the unfairness score should be calculated to assess the fairness of the model. At the end, a specific bayesian framework for calculating model uncertainty and fairness uncertainty should be provided.

\subsection{RQ3: Definition of fairness}

This will be discussed further in section~\ref{ch:related}, but given the different definitions of fairness, investigation on which one is more appropriate should be done. At the end, a definition of choice should be presented.

\subsection{RQ4: How does the probabilitic approach compare with traditional machine learning methods?}

The probabilistic model proposed in this thesis should be compared to a baseline method to see if there are any benefits in terms of performance, accuracy, fairness as well as addin the bayesian persepctive to the problem. At the end, simulations should be presented with both real world datasets and synthetic datasets and their repsective performance metrics.


\instructions{
\begin{itemize}
   \item Define the goals of your study. It might be presented as a bullet list.
  \item Structure your goal by Research Questions (RQs). Describe each of the problems to address in your work and formulate for it a clear research question.``The problem of bla is about$\dots$, We formulate the following RQ1 Can we provide a method for bla such that bla bla''
\end{itemize}
}

\section{Approach and Contributions}
\label{sec:intro:approach}

In this thesis, different algorithms and methods have been explored. These have all been made into a python package for ease of use. 

\instructions{
\begin{itemize}
  \item Give a brief summary of your overall approach.
  \item Summarize the specific contributions that you made in this thesis (e.g., a task definition, a method or model, a test collection, empirical results, analysis, etc.). It might be presented as a bullet list.
\end{itemize}
}

\section{Outline}
\label{sec:intro:outline}

\instructions{
\begin{itemize}
    \item Give an overview of the main points and the structure of your thesis. ``Chapter 2 covers ... Chapter 3 describes ... ''
    \item Show in a natural way how the different parts (chapters) relate to each other.
\end{itemize}
}